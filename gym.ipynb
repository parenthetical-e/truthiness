{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gym\n",
    "> A `Gym` enviroment for teaching truth and conseqences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "from copy import deepcopy\n",
    "from gym import spaces\n",
    "from gym.utils import seeding\n",
    "from itertools import cycle\n",
    "\n",
    "from truthiness.game import create_maze\n",
    "from truthiness.game import shame_game\n",
    "from truthiness.game import plain_game\n",
    "from truthiness.game import available_moves\n",
    "from truthiness.game import random_move\n",
    "\n",
    "# Gym is annoying these days...\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# the base\n",
    "First we define a `Base` game class. It has methods that are common between all the games we will want to play. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Base(gym.Env):    \n",
    "    def moves(self):\n",
    "        \"\"\"Return all available moves\"\"\"\n",
    "        # Get all the moves then filter for moves that\n",
    "        # have already been played\n",
    "        candidates = available_moves(self.x, self.y, self.maze)\n",
    "        \n",
    "        available = []\n",
    "        for a in candidates:\n",
    "            if a not in self.move_history:\n",
    "                available.append(a)\n",
    "                \n",
    "        return available\n",
    "    \n",
    "    \n",
    "    def render(self, mode='human', close=False):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# the shame game env\n",
    "The details of this game are described in the `boards` file. This module exists only to put the games described there into a [gym](https://github.com/openai/gym) environment, and then do some simple testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class ShameGame1(Base):\n",
    "    \"\"\"A one-sided game of learning and shame\"\"\"\n",
    "    def __init__(self, maze=None, sigma=0.5, shame=0.5, max_steps=10):\n",
    "        self.maze = maze\n",
    "        self.n = self.maze.shape[0]\n",
    "        self.max_steps = max_steps\n",
    "        \n",
    "        self.sigma = sigma\n",
    "        self.shame = shame\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    \n",
    "    def step(self, move):\n",
    "        if self.count > self.max_steps:\n",
    "            raise ValueError(f\"env exceeded max_steps ({self.count})\")\n",
    "\n",
    "        # Shuffle state, and generate returns\n",
    "        self.x, self.y = move\n",
    "        self.move_history.append(move)\n",
    "        \n",
    "        # Values are only found once\n",
    "        reward = deepcopy((self.E[x,y], self.Q[x,y]))\n",
    "        self.E[x,y] = 0\n",
    "        self.Q[x,y] = 0\n",
    "        state = (self.y, self.x, self.E, self.Q)\n",
    "\n",
    "        # Limit game length\n",
    "        self.count += 1\n",
    "        if self.count >= self.max_steps:\n",
    "            self.done = True\n",
    "            \n",
    "        return state, reward, self.done, {}\n",
    "\n",
    "    \n",
    "    def reset(self):\n",
    "        # reinit\n",
    "        self.count = 0\n",
    "        self.done = False\n",
    "        self.move_history = []\n",
    "        \n",
    "        # Generate new  \n",
    "        self.x, self.y = random_move(self.maze)\n",
    "        self.E, self.Q = shame_game(\n",
    "            self.n, sigma=self.sigma, shame=self.shame, maze=self.maze)\n",
    "        \n",
    "        return (self.y, self.x, self.E, self.Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Example of random play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maze = create_maze(8, k=5, t=10)\n",
    "env = ShameGame1(maze=maze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 3 (0.16023463499991197, 0.08011731749995599) False\n",
      "0 3 (0.4676921743912892, 0.2338460871956446) False\n",
      "0 5 (0.23516501732895362, 0.11758250866447681) False\n",
      "4 5 (0.4613412681151665, 0.23067063405758326) False\n",
      "2 5 (0.33482306926727334, 0.16741153463363667) False\n",
      "3 5 (0.22466932311115803, 0.11233466155557902) False\n",
      "1 5 (0.604971504844737, 0.3024857524223685) False\n",
      "1 4 (0.4868102183235079, 0.24340510916175395) False\n",
      "1 7 (0.8244807104702709, 0.41224035523513547) False\n",
      "0 7 (0.3923635388931987, 0.19618176944659935) True\n"
     ]
    }
   ],
   "source": [
    "done = False\n",
    "x, y, Q, E = env.reset()\n",
    "\n",
    "moves = [(x, y)]\n",
    "while not done:\n",
    "    available = env.moves()    \n",
    "    i = np.random.randint(0, len(available))\n",
    "    x, y = available[i]\n",
    "    state, reward, done, _ = env.step((x, y))\n",
    "    moves.append((x, y))\n",
    "    \n",
    "    print(x, y, reward, done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0), (0, 1), (0, 0)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# the plain game env\n",
    "As above the details of this game are described in the `boards` file. This module exists only to put the games described there into a [gym](https://github.com/openai/gym) environment, and then do some simple testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class PlainGame1(Base):\n",
    "    \"\"\"A one-sided game of learning and consequences\"\"\"\n",
    "    def __init__(self, maze=None, sigma=0.5, max_steps=10):\n",
    "        self.maze = maze\n",
    "        self.n = self.maze.shape[0]\n",
    "        self.max_steps = max_steps\n",
    "        \n",
    "        self.sigma = sigma\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    \n",
    "    def step(self, move):\n",
    "        if self.count > self.max_steps:\n",
    "            raise ValueError(f\"env exceeded max_steps ({self.count})\")\n",
    "\n",
    "        # Shuffle state, and generate returns\n",
    "        self.x, self.y = move\n",
    "        self.move_history.append(move)\n",
    "        \n",
    "        # Values are only found once\n",
    "        reward = deepcopy((self.E[x,y], self.Q[x,y]))\n",
    "        self.E[x,y] = 0\n",
    "        self.Q[x,y] = 0\n",
    "        state = (self.y, self.x, self.E, self.Q)\n",
    "\n",
    "        \n",
    "        # Limit game length\n",
    "        self.count += 1\n",
    "        if self.count >= self.max_steps:\n",
    "            self.done = True\n",
    "            \n",
    "        return state, reward, self.done, {}\n",
    "\n",
    "    \n",
    "    def reset(self):\n",
    "        # reinit\n",
    "        self.count = 0\n",
    "        self.done = False\n",
    "        self.move_history = []\n",
    "        \n",
    "        # Generate new  \n",
    "        self.x, self.y = random_move(self.maze)\n",
    "        self.E, self.Q = plain_game(self.n, sigma=self.sigma, maze=self.maze)\n",
    "        \n",
    "        return (self.y, self.x, self.E, self.Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Example of random play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 6 (0.8173955708928679, 0.3678441608119558) False\n",
      "3 6 (0.2120080399911605, 0.3317761013275871) False\n",
      "3 4 (0.16630064549786405, 0.3271338472625862) False\n",
      "1 4 (0.4227259498212686, 0.2947552064684267) False\n",
      "1 6 (0.14824614442997494, 0.1950148362587161) False\n",
      "1 3 (0.1421543836769066, 0.8314389725646877) False\n",
      "1 7 (0.17567547348478108, 0.40232919491499175) False\n",
      "2 7 (0.22003040387953465, 0.7762403705385325) False\n",
      "3 7 (0.6779770834936715, 0.1637790976810524) False\n",
      "6 7 (0.18727384631048136, 0.16384510426435447) True\n"
     ]
    }
   ],
   "source": [
    "maze = create_maze(8, k=5, t=10)\n",
    "env = PlainGame1(maze=maze)\n",
    "\n",
    "done = False\n",
    "x, y, Q, E = env.reset()\n",
    "\n",
    "moves = [(x, y)]\n",
    "while not done:\n",
    "    available = env.moves()    \n",
    "    i = np.random.randint(0, len(available))\n",
    "    x, y = available[i]\n",
    "    state, reward, done, _ = env.step((x, y))\n",
    "    moves.append((x, y))\n",
    "    \n",
    "    print(x, y, reward, done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
