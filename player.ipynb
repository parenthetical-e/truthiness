{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp player"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# player\n",
    "> Mathmatical players for games of truth and consequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import numpy as np\n",
    "from truthiness import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# a run function\n",
    "First, we make a run function to run our players. \n",
    "- An 'episode' is a full game of play. \n",
    "- Each episode uses a different random seed.\n",
    "- Data for each game gets logged to csv file. \n",
    "- (The loging interface is mine but is compatible with the [tensorboard](https://www.tensorflow.org/tensorboard) logging system for deep learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export \n",
    "def run(player, num_episode=10, env_name=\"ShameGame1\", **env_kwargs):\n",
    "    \"\"\"Run a set of games\"\"\"\n",
    "    \n",
    "    # Get the env\n",
    "    Env = getattr(env_name, gym)\n",
    "    \n",
    "    # Init logging\n",
    "    \n",
    "    # !\n",
    "    for n in range(num_episodes):\n",
    "        # Reconfig the env\n",
    "        maze = create_maze(8, k=5, t=10)\n",
    "        env = Env(maze=maze, **env_kwargs)\n",
    "        \n",
    "        # Reset\n",
    "        x, y, Q, E = env.reset()\n",
    "        done = False\n",
    "\n",
    "        # -\n",
    "        while not done:\n",
    "            x, y = player(E, Q, env.moves())\n",
    "            state, reward, done, _ = env.step((x, y))    \n",
    "            x, y, Q, E = state\n",
    "            \n",
    "        # Log data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# all players "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define different simple players and dispositions to truth and consequences. \n",
    "- Honesty: $max\\ E_{i,j}$\n",
    "- Planned honesty: $argmax_{\\pi} \\sum_P E_{i,j}$\n",
    "- Sensitive: $min\\ Q_{i,j}$\n",
    "- Planned sensitive: $argmin_{\\pi} \\sum_P Q_{i,j}$\n",
    "- Evil: $max\\ Q_{i,j}$\n",
    "- Planned evil: $argmax_{\\pi} \\sum_P Q_{i,j}$\n",
    "- Foraging: $max\\ \\frac{E_{i,j}}{Q_{i,j}}$\n",
    "- Planned foraging: $argmax_{\\pi} \\sum_P \\frac{E_{i,j}}{Q_{i,j}}$\n",
    "- Random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also consider a couple more complex approaches which swtich amoung the simple policies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# a random player\n",
    "They play by making random moves. Simple and bad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class Random:\n",
    "    def __init__(self, prng=None):\n",
    "        if prng is None:\n",
    "            self.prng = np.random.RandomState()\n",
    "        else:\n",
    "            self.prng = prng\n",
    "    \n",
    "    def __call__(self, moves):\n",
    "        return self.forward(moves)\n",
    "\n",
    "    def forward(self, E, Q, moves):\n",
    "        i = self.prng.randint(0, len(moves))\n",
    "        move = moves[i]\n",
    "        \n",
    "        return move"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
