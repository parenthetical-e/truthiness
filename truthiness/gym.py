# AUTOGENERATED! DO NOT EDIT! File to edit: gym.ipynb (unless otherwise specified).

__all__ = ['Base', 'ShameGame1', 'PlainGame1']

# Cell
import numpy as np
import gym

from copy import deepcopy
from gym import spaces
from gym.utils import seeding
from itertools import cycle

from .game import create_maze
from .game import shame_game
from .game import plain_game
from .game import available_moves
from .game import random_move

# Gym is annoying these days...
import warnings

warnings.filterwarnings("ignore")

# Cell
class Base(gym.Env):
    def moves(self):
        """Return all available moves"""
        # Get all the moves then filter for moves that
        # have already been played
        candidates = available_moves(self.x, self.y, self.maze)

        available = []
        for a in candidates:
            if a not in self.move_history:
                available.append(a)

        return available

    def set_maze(self, maze):
        self.maze = maze

    def render(self, mode="human", close=False):
        pass

# Cell
class ShameGame1(Base):
    """A one-sided game of learning and shame"""

    def __init__(self, n, maze=None, sigma=0.5, shame=0.5, max_steps=10, seed=None):
        self.n = n
        self.maze = maze
        self.max_steps = max_steps
        self.sigma = sigma
        self.shame = shame

        self.prng = np.random.RandomState(seed)
        self.reset()

    def step(self, move):
        if self.count > self.max_steps:
            raise ValueError(f"env exceeded max_steps ({self.count})")

        # Shuffle state, and generate returns
        x, y = move
        self.move_history.append(move)

        # Values are only found once
        reward = deepcopy((self.E[x, y], self.Q[x, y]))
        self.E[x, y] = 0
        self.Q[x, y] = 0
        self.x, self.y = x, y
        state = (self.y, self.x, self.E, self.Q)

        # Limit game length
        self.count += 1
        if self.count >= self.max_steps:
            self.done = True

        return state, reward, self.done, {}

    def reset(self):
        # reinit
        self.count = 0
        self.done = False
        self.move_history = []

        # Generate new
        self.x, self.y, self.prng = random_move(self.maze, prng=self.prng)
        self.E, self.Q, self.prng = shame_game(
            self.n, sigma=self.sigma, shame=self.shame, maze=self.maze, prng=self.prng
        )

        return (self.y, self.x, self.E, self.Q)

# Cell
class PlainGame1(Base):
    """A one-sided game of learning and consequences"""

    def __init__(self, n, maze=None, sigma=0.5, max_steps=10, seed=None):
        self.n = n
        self.maze = maze
        self.max_steps = max_steps
        self.sigma = sigma

        self.prng = np.random.RandomState(seed)
        self.reset()

    def step(self, move):
        if self.count > self.max_steps:
            raise ValueError(f"env exceeded max_steps ({self.count})")

        # Shuffle state, and generate returns
        x, y = move
        self.move_history.append(move)

        # Values are only found once
        reward = deepcopy((self.E[x, y], self.Q[x, y]))
        self.E[x, y] = 0
        self.Q[x, y] = 0
        self.x, self.y = x, y
        state = (self.y, self.x, self.E, self.Q)

        # Limit game length
        self.count += 1
        if self.count >= self.max_steps:
            self.done = True

        return state, reward, self.done, {}

    def reset(self):
        # reinit
        self.count = 0
        self.done = False
        self.move_history = []

        # Generate new
        self.x, self.y, self.prng = random_move(self.maze, self.prng)
        self.E, self.Q, self.prng = plain_game(
            self.n, sigma=self.sigma, maze=self.maze, prng=self.prng
        )

        return (self.y, self.x, self.E, self.Q)