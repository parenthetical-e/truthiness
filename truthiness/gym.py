# AUTOGENERATED! DO NOT EDIT! File to edit: gym.ipynb (unless otherwise specified).

__all__ = ['ShameGame1', 'PlainGame1']

# Cell
import numpy as np
import gym

from copy import deepcopy
from gym import spaces
from gym.utils import seeding
from itertools import cycle

from .game import create_maze
from .game import shame_game
from .game import plain_game
from .game import available_moves
from .game import random_move

# Gym is annoying these days...
import warnings
warnings.filterwarnings("ignore")

# Cell
class ShameGame1(Base):
    """A one-sided game of learning and shame"""
    def __init__(self, maze=None, sigma=0.5, shame=0.5, max_steps=10):
        self.maze = maze
        self.n = self.maze.shape[0]
        self.max_steps = max_steps

        self.sigma = sigma
        self.shame = shame

        self.reset()


    def step(self, move):
        if self.count > self.max_steps:
            raise ValueError(f"env exceeded max_steps ({self.count})")

        # Shuffle state, and generate returns
        self.x, self.y = move
        self.move_history.append(move)

        # Values are only found once
        reward = deepcopy((self.E[x,y], self.Q[x,y]))
        self.E[x,y] = 0
        self.Q[x,y] = 0
        state = (self.y, self.x, self.E, self.Q)

        # Limit game length
        self.count += 1
        if self.count >= self.max_steps:
            self.done = True

        return state, reward, self.done, {}


    def reset(self):
        # reinit
        self.count = 0
        self.done = False
        self.move_history = []

        # Generate new
        self.x, self.y = random_move(self.maze)
        self.E, self.Q = shame_game(
            self.n, sigma=self.sigma, shame=self.shame, maze=self.maze)

        return (self.y, self.x, self.E, self.Q)

# Cell
class PlainGame1(Base):
    """A one-sided game of learning and consequences"""
    def __init__(self, maze=None, sigma=0.5, max_steps=10):
        self.maze = maze
        self.n = self.maze.shape[0]
        self.max_steps = max_steps

        self.sigma = sigma

        self.reset()


    def step(self, move):
        if self.count > self.max_steps:
            raise ValueError(f"env exceeded max_steps ({self.count})")

        # Shuffle state, and generate returns
        self.x, self.y = move
        self.move_history.append(move)

        # Values are only found once
        reward = deepcopy((self.E[x,y], self.Q[x,y]))
        self.E[x,y] = 0
        self.Q[x,y] = 0
        state = (self.y, self.x, self.E, self.Q)


        # Limit game length
        self.count += 1
        if self.count >= self.max_steps:
            self.done = True

        return state, reward, self.done, {}


    def reset(self):
        # reinit
        self.count = 0
        self.done = False
        self.move_history = []

        # Generate new
        self.x, self.y = random_move(self.maze)
        self.E, self.Q = plain_game(self.n, sigma=self.sigma, maze=self.maze)

        return (self.y, self.x, self.E, self.Q)